{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending Vocabuary with n_gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is n-gram?\n",
    "- An n-gram is a sequence containing up to n elements that heave been extracted from the sequence of those elements, ususally a string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why we have to use n-gram?\n",
    "- You saw in previous notebook that when a sequence fo tokens are vectorized into bag-of-words vector, it loses a lot of meaning inherent in the order of those words.\n",
    "- e.g \"Chocolate Cake\" has the more meaningfull than only \"Chocolate\" and \"Cake\".\n",
    "- Sp we use n-grams to incease the meaning of the sequenced token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \" I know you are interested in natural language processing. I believe that this is a very interesting topic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentence = word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'know',\n",
       " 'you',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'I',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'interesting',\n",
       " 'topic',\n",
       " '.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'know'),\n",
       " ('know', 'you'),\n",
       " ('you', 'are'),\n",
       " ('are', 'interested'),\n",
       " ('interested', 'in'),\n",
       " ('in', 'natural'),\n",
       " ('natural', 'language'),\n",
       " ('language', 'processing'),\n",
       " ('processing', '.'),\n",
       " ('.', 'I'),\n",
       " ('I', 'believe'),\n",
       " ('believe', 'that'),\n",
       " ('that', 'this'),\n",
       " ('this', 'is'),\n",
       " ('is', 'a'),\n",
       " ('a', 'very'),\n",
       " ('very', 'interesting'),\n",
       " ('interesting', 'topic'),\n",
       " ('topic', '.')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now see what is happen when we use ngrams\n",
    "# ngram provide a generator so must convert it to a list to see the result\n",
    "\n",
    "two_grams = list(ngrams(tokenized_sentence, 2))\n",
    "two_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'know', 'you'),\n",
       " ('know', 'you', 'are'),\n",
       " ('you', 'are', 'interested'),\n",
       " ('are', 'interested', 'in'),\n",
       " ('interested', 'in', 'natural'),\n",
       " ('in', 'natural', 'language'),\n",
       " ('natural', 'language', 'processing'),\n",
       " ('language', 'processing', '.'),\n",
       " ('processing', '.', 'I'),\n",
       " ('.', 'I', 'believe'),\n",
       " ('I', 'believe', 'that'),\n",
       " ('believe', 'that', 'this'),\n",
       " ('that', 'this', 'is'),\n",
       " ('this', 'is', 'a'),\n",
       " ('is', 'a', 'very'),\n",
       " ('a', 'very', 'interesting'),\n",
       " ('very', 'interesting', 'topic'),\n",
       " ('interesting', 'topic', '.')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "three_grams = list(ngrams(tokenized_sentence, 3))\n",
    "three_grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you see the impact of ngrams on the tokens. It is more meaningfull to use ngrams when you have sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I know',\n",
       " 'know you',\n",
       " 'you are',\n",
       " 'are interested',\n",
       " 'interested in',\n",
       " 'in natural',\n",
       " 'natural language',\n",
       " 'language processing',\n",
       " 'processing .',\n",
       " '. I',\n",
       " 'I believe',\n",
       " 'believe that',\n",
       " 'that this',\n",
       " 'this is',\n",
       " 'is a',\n",
       " 'a very',\n",
       " 'very interesting',\n",
       " 'interesting topic',\n",
       " 'topic .']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join(gram) for gram in two_grams]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you inspect the ngrams token you will see that there are tokens such as \"that this\", \"this is\" , \"is a\",\"I am\" type tokens which are very common in nature and are not very helpful to extract meaning of the documents.\n",
    "\n",
    "- So, if a token or n-gram occurs in more that 25% of all the documents in corpus we ignore it.\n",
    "- on the other hand, if a token or n-gram occurs in less than 5% of all the documents in corpus we ignore it.\n",
    "\n",
    "By above process we can reduce the dimension of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop words are words that are not helpful to extract meaning of the documents. and are very very common in nature.\n",
    "\n",
    "such as \"a\", \"an\", \"the\", prepositions, articles, pronouns, conjunctions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Avijit\n",
      "[nltk_data]     Biswas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words = nltk.corpus.stopwords.words('english')\n",
    "len(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS as skl_stopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skl_stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize your vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural', 'language', 'processing']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"Natural\", \"Language\", \"Processing\"]\n",
    "case_norm_toks = [x.lower() for x in tokens]\n",
    "case_norm_toks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'know',\n",
       " 'you',\n",
       " 'are',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '.',\n",
       " 'i',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'this',\n",
       " 'is',\n",
       " 'a',\n",
       " 'very',\n",
       " 'interesting',\n",
       " 'topic',\n",
       " '.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "case_norm_tokens = [x.lower() for x in tokenized_sentence]\n",
    "case_norm_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we learn about Term-Frequency and Inverse Document Frequency or (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e0c0fc535d3a4574cc8f7de5fd7fa08a6aadbe1efe276a04e27ee4584e1eb9fa"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
