{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _Natural Language Processing_\n",
    "## _Building Vocabulary using tokenizer_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'am',\n",
       " 'learning',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'with',\n",
       " 'python',\n",
       " 'in',\n",
       " '2022.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"I am learning natural language processing with python in 2022.\"\n",
    "# tokenize the sentence\n",
    "tokenized_sentence = sentence.split()\n",
    "tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python built-in function str.split() did a good job to tokenize the sentence. but if you look makes a mistakes at last word, it includes the sentence ending punctuation with the token \"_2022._\"\n",
    "\n",
    "A good tokenizer will be able to tokenize the sentence correctly.Means it will not include the sentence ending punctuation with the token \"_2022_\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets forget the mistakes we will deal with it later in more advance phase. \n",
    "\n",
    "Now we will turning the words into vector representation by doing one-hot operation on the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of unique words\n",
    "vocab = sorted(set(tokenized_sentence))\n",
    "\n",
    "# Why I use set? \n",
    "# Because I want to remove the duplicate words in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2022., I, am, in, language, learning, natural, processing, python, with'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the words in the vocab list\n",
    "', '.join(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tokens: 10\n",
      "Vocab Size: 10\n"
     ]
    }
   ],
   "source": [
    "num_tokens = len(tokenized_sentence)\n",
    "vocab_size = len(vocab)\n",
    "print(f\"Number of Tokens: {num_tokens}\\nVocab Size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_encoding = np.zeros((num_tokens, vocab_size), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "onehot_encoding is just a matrix of num_token x vocab_size.\n",
    "- num_tokens = Rows\n",
    "- vocab_size = Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now convert tokens to one-hot encoding\n",
    "for i, token in enumerate(tokenized_sentence):\n",
    "    index = vocab.index(token)\n",
    "    onehot_encoding[i, index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is just happen here?\n",
    "- for each words in the sentence, we will find the index of the word in the vocab_size.\n",
    "- then mark the columns for that word in the vocabulary with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022. I am in language learning natural processing python with\n",
      "[[0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\" \".join(vocab))\n",
    "print(onehot_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2022.</th>\n",
       "      <th>I</th>\n",
       "      <th>am</th>\n",
       "      <th>in</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "      <th>python</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   2022.  I  am  in  language  learning  natural  processing  python  with\n",
       "0      0  1   0   0         0         0        0           0       0     0\n",
       "1      0  0   1   0         0         0        0           0       0     0\n",
       "2      0  0   0   0         0         1        0           0       0     0\n",
       "3      0  0   0   0         0         0        1           0       0     0\n",
       "4      0  0   0   0         1         0        0           0       0     0\n",
       "5      0  0   0   0         0         0        0           1       0     0\n",
       "6      0  0   0   0         0         0        0           0       0     1\n",
       "7      0  0   0   0         0         0        0           0       1     0\n",
       "8      0  0   0   1         0         0        0           0       0     0\n",
       "9      1  0   0   0         0         0        0           0       0     0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets view it using Pandas\n",
    "import pandas as pd\n",
    "pd.DataFrame(onehot_encoding,columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot vectors are super-sparse matrix, you can see there are lots of zeros üò∂ in the matrix. So, working with one hot vector can be a dimensional problem.üò• \n",
    "\n",
    "e.g if you have a sentence of length 100, or a full novel, you will have a matrix of 100 x vocab_sizeüò® or total_number_of_words_in_novel x vocab_size.üò±\n",
    "\n",
    "Ok, I must give you idea about the size and space it required.\n",
    "\n",
    "**LOOK üëá**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 14400000\n",
      "Bytes: 14400000000000\n",
      "GB: 13411.04507446289gb\n",
      "TB: 13.096723705530167tb\n",
      "SO, you have 13tb of data from a single corpus. That's a lot of data.\n"
     ]
    }
   ],
   "source": [
    "# lets you have 300 books with 4000 sentence each and 12 words per line.\n",
    "# then, \n",
    "rows = 300 * 4000 * 12\n",
    "num_bytes = rows * 1000000\n",
    "\n",
    "in_gb = num_bytes / (1024 * 1024 * 1024)\n",
    "in_tb = in_gb / 1024\n",
    "\n",
    "print(f\"Rows: {rows}\")\n",
    "print(f\"Bytes: {num_bytes}\")\n",
    "print(f\"GB: {in_gb}gb\")\n",
    "print(f\"TB: {in_tb}tb\")\n",
    "\n",
    "print(\"SO, you have 13tb of data from a single corpus. That's a lot of data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2022.</th>\n",
       "      <th>I</th>\n",
       "      <th>am</th>\n",
       "      <th>in</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>natural</th>\n",
       "      <th>processing</th>\n",
       "      <th>python</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  2022.  I am in language learning natural processing python with\n",
       "0        1                                                       \n",
       "1           1                                                    \n",
       "2                                1                               \n",
       "3                                        1                       \n",
       "4                       1                                        \n",
       "5                                                   1            \n",
       "6                                                               1\n",
       "7                                                          1     \n",
       "8              1                                                 \n",
       "9     1                                                          "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# will you see the actual sentence? in matrix form üòâ\n",
    "# ok , but don't do it with the dataframe for ML algorithms\n",
    "df = pd.DataFrame(onehot_encoding,columns=vocab)\n",
    "df[df==0] = \" \"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to tackle the problem of one-hot vector?\n",
    "- there are many ways to tackle the problems , one is use of _Bag-of-Words_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Bag of Words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_bow = {}\n",
    "for token in sentence.split():\n",
    "    if token not in sentence_bow:\n",
    "        sentence_bow[token] = 1\n",
    "    else:\n",
    "        sentence_bow[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('2022.', 1),\n",
       " ('I', 1),\n",
       " ('am', 1),\n",
       " ('in', 1),\n",
       " ('language', 1),\n",
       " ('learning', 1),\n",
       " ('natural', 1),\n",
       " ('processing', 1),\n",
       " ('python', 1),\n",
       " ('with', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sentence_bow.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- You noticed that sorted function put decimal number before the words.\n",
    "- and put capitalized words before the lowercase words.\n",
    "- why?\n",
    "  - Because this is the ordering of the characters in the ASCII and Unicode character sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I             1\n",
       "am            1\n",
       "learning      1\n",
       "natural       1\n",
       "language      1\n",
       "processing    1\n",
       "with          1\n",
       "python        1\n",
       "in            1\n",
       "2022.         1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: dictionary to pandas series\n",
    "pd.Series(dict([(token, 1) for token in sentence.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>I</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>am</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>natural</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>language</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processing</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>with</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>python</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>in</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022.</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            0\n",
       "I           1\n",
       "am          1\n",
       "learning    1\n",
       "natural     1\n",
       "language    1\n",
       "processing  1\n",
       "with        1\n",
       "python      1\n",
       "in          1\n",
       "2022.       1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 2: pandas series to DataFrame\n",
    "pd.DataFrame(pd.Series(dict([(token, 1) for token in sentence.split()])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>am</th>\n",
       "      <th>learning</th>\n",
       "      <th>natural</th>\n",
       "      <th>language</th>\n",
       "      <th>processing</th>\n",
       "      <th>with</th>\n",
       "      <th>python</th>\n",
       "      <th>in</th>\n",
       "      <th>2022.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   I  am  learning  natural  language  processing  with  python  in  2022.\n",
       "0  1   1         1        1         1           1     1       1   1      1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 3: Transpose the DataFrame\n",
    "pd.DataFrame(pd.Series(dict([(token, 1) for token in sentence.split()]))).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do it altogather.\n",
    "üëá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>I</th>\n",
       "      <th>am</th>\n",
       "      <th>learning</th>\n",
       "      <th>natural</th>\n",
       "      <th>language</th>\n",
       "      <th>processing</th>\n",
       "      <th>with</th>\n",
       "      <th>python</th>\n",
       "      <th>in</th>\n",
       "      <th>2022.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       I  am  learning  natural  language  processing  with  python  in  2022.\n",
       "token  1   1         1        1         1           1     1       1   1      1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets use pandas efficient DataFrame to store the data\n",
    "df_sentence = pd.DataFrame(pd.Series(dict([(token,1) for token in sentence.split()])), columns=['token']).T\n",
    "\n",
    "df_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct a dataframe of Bag of Words vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime and Punishment from Project Gutenberg\n",
    "\n",
    "novel = \"\"\"On an exceptionally hot evening early in July a young man came out of\n",
    "the garret in which he lodged in S. Place and walked slowly, as though\n",
    "in hesitation, towards K. bridge.\n",
    "\n",
    "He had successfully avoided meeting his landlady on the staircase. His\n",
    "garret was under the roof of a high, five-storied house and was more\n",
    "like a cupboard than a room. The landlady who provided him with garret,\n",
    "dinners, and attendance, lived on the floor below, and every time\n",
    "he went out he was obliged to pass her kitchen, the door of which\n",
    "invariably stood open. And each time he passed, the young man had a\n",
    "sick, frightened feeling, which made him scowl and feel ashamed. He was\n",
    "hopelessly in debt to his landlady, and was afraid of meeting her.\n",
    "\n",
    "This was not because he was cowardly and abject, quite the contrary; but\n",
    "for some time past he had been in an overstrained irritable condition,\n",
    "verging on hypochondria. He had become so completely absorbed in\n",
    "himself, and isolated from his fellows that he dreaded meeting, not\n",
    "only his landlady, but anyone at all. He was crushed by poverty, but the\n",
    "anxieties of his position had of late ceased to weigh upon him. He had\n",
    "given up attending to matters of practical importance; he had lost all\n",
    "desire to do so. Nothing that any landlady could do had a real terror\n",
    "for him. But to be stopped on the stairs, to be forced to listen to her\n",
    "trivial, irrelevant gossip, to pestering demands for payment, threats\n",
    "and complaints, and to rack his brains for excuses, to prevaricate, to\n",
    "lie--no, rather than that, he would creep down the stairs like a cat and\n",
    "slip out unseen.\n",
    "\n",
    "This evening, however, on coming out into the street, he became acutely\n",
    "aware of his fears.\n",
    "\n",
    "‚ÄúI want to attempt a thing _like that_ and am frightened by these\n",
    "trifles,‚Äù he thought, with an odd smile. ‚ÄúHm... yes, all is in a man‚Äôs\n",
    "hands and he lets it all slip from cowardice, that‚Äôs an axiom. It would\n",
    "be interesting to know what it is men are most afraid of. Taking a new\n",
    "step, uttering a new word is what they fear most.... But I am talking\n",
    "too much. It‚Äôs because I chatter that I do nothing. Or perhaps it is\n",
    "that I chatter because I do nothing. I‚Äôve learned to chatter this\n",
    "last month, lying for days together in my den thinking... of Jack the\n",
    "Giant-killer. Why am I going there now? Am I capable of _that_? Is\n",
    "_that_ serious? It is not serious at all. It‚Äôs simply a fantasy to amuse\n",
    "myself; a plaything! Yes, maybe it is a plaything.‚Äù\n",
    "\n",
    "The heat in the street was terrible: and the airlessness, the bustle\n",
    "and the plaster, scaffolding, bricks, and dust all about him, and that\n",
    "special Petersburg stench, so familiar to all who are unable to get out\n",
    "of town in summer--all worked painfully upon the young man‚Äôs already\n",
    "overwrought nerves. The insufferable stench from the pot-houses, which\n",
    "are particularly numerous in that part of the town, and the drunken men\n",
    "whom he met continually, although it was a working day, completed\n",
    "the revolting misery of the picture. An expression of the profoundest\n",
    "disgust gleamed for a moment in the young man‚Äôs refined face. He was,\n",
    "by the way, exceptionally handsome, above the average in height, slim,\n",
    "well-built, with beautiful dark eyes and dark brown hair. Soon he sank\n",
    "into deep thought, or more accurately speaking into a complete blankness\n",
    "of mind; he walked along not observing what was about him and not caring\n",
    "to observe it. From time to time, he would mutter something, from the\n",
    "habit of talking to himself, to which he had just confessed. At these\n",
    "moments he would become conscious that his ideas were sometimes in a\n",
    "tangle and that he was very weak; for two days he had scarcely tasted\n",
    "food.\n",
    "He was so badly dressed that even a man accustomed to shabbiness would\n",
    "have been ashamed to be seen in the street in such rags. In that quarter\n",
    "of the town, however, scarcely any shortcoming in dress would have\n",
    "created surprise. Owing to the proximity of the Hay Market, the number\n",
    "of establishments of bad character, the preponderance of the trading\n",
    "and working class population crowded in these streets and alleys in the\n",
    "heart of Petersburg, types so various were to be seen in the streets\n",
    "that no figure, however queer, would have caused surprise. But there was\n",
    "such accumulated bitterness and contempt in the young man‚Äôs heart, that,\n",
    "in spite of all the fastidiousness of youth, he minded his rags least\n",
    "of all in the street. It was a different matter when he met with\n",
    "acquaintances or with former fellow students, whom, indeed, he disliked\n",
    "meeting at any time. And yet when a drunken man who, for some unknown\n",
    "reason, was being taken somewhere in a huge waggon dragged by a heavy\n",
    "dray horse, suddenly shouted at him as he drove past: ‚ÄúHey there, German\n",
    "hatter‚Äù bawling at the top of his voice and pointing at him--the young\n",
    "man stopped suddenly and clutched tremulously at his hat. It was a tall\n",
    "round hat from Zimmerman‚Äôs, but completely worn out, rusty with age, all\n",
    "torn and bespattered, brimless and bent on one side in a most unseemly\n",
    "fashion. Not shame, however, but quite another feeling akin to terror\n",
    "had overtaken him.\n",
    "\n",
    "‚ÄúI knew it,‚Äù he muttered in confusion, ‚ÄúI thought so! That‚Äôs the worst\n",
    "of all! Why, a stupid thing like this, the most trivial detail might\n",
    "spoil the whole plan. Yes, my hat is too noticeable.... It looks absurd\n",
    "and that makes it noticeable.... With my rags I ought to wear a cap, any\n",
    "sort of old pancake, but not this grotesque thing. Nobody wears such\n",
    "a hat, it would be noticed a mile off, it would be remembered.... What\n",
    "matters is that people would remember it, and that would give them\n",
    "a clue. For this business one should be as little conspicuous as\n",
    "possible.... Trifles, trifles are what matter! Why, it‚Äôs just such\n",
    "trifles that always ruin everything....‚Äù\n",
    "\n",
    "He had not far to go; he knew indeed how many steps it was from the gate\n",
    "of his lodging house: exactly seven hundred and thirty. He had counted\n",
    "them once when he had been lost in dreams. At the time he had put no\n",
    "faith in those dreams and was only tantalising himself by their hideous\n",
    "but daring recklessness. Now, a month later, he had begun to look upon\n",
    "them differently, and, in spite of the monologues in which he jeered at\n",
    "his own impotence and indecision, he had involuntarily come to regard\n",
    "this ‚Äúhideous‚Äù dream as an exploit to be attempted, although he\n",
    "still did not realise this himself. He was positively going now for a\n",
    "‚Äúrehearsal‚Äù of his project, and at every step his excitement grew more\n",
    "and more violent.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"On an exceptionally hot evening early in July a young man came out of the garret in which he lodged in S.\\n\"\"\"\n",
    "sentence += \"\"\"Place and walked slowly, as though in hesitation, towards K. bridge.\\n\"\"\"\n",
    "sentence += \"\"\"He had successfully avoided meeting his landlady on the staircase.\\n\"\"\"\n",
    "sentence += \"\"\"His garret was under the roof of a high, five-storied house and was more\n",
    "like a cupboard than a room.\\n\"\"\"\n",
    "sentence += \"\"\"The landlady who provided him with garret, dinners, and attendance, lived on the floor below, and every time\n",
    "he went out he was obliged to pass her kitchen, the door of which invariably stood open.\\n\"\"\"\n",
    "\n",
    "sentence += \"\"\"And each time he passed, the young man had a sick, frightened feeling, which made him scowl and feel ashamed.\\n\"\"\"\n",
    "\n",
    "sentence += \"\"\"He was hopelessly in debt to his landlady, and was afraid of meeting her.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = {}\n",
    "for i, sent in enumerate(sentence.split('\\n')):\n",
    "    corpus['sent{}'.format(i)] = dict((tok, 1) for tok in sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = pd.DataFrame.from_records(corpus).fillna(0).astype(int).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>On</th>\n",
       "      <th>an</th>\n",
       "      <th>exceptionally</th>\n",
       "      <th>hot</th>\n",
       "      <th>evening</th>\n",
       "      <th>early</th>\n",
       "      <th>in</th>\n",
       "      <th>July</th>\n",
       "      <th>a</th>\n",
       "      <th>young</th>\n",
       "      <th>man</th>\n",
       "      <th>came</th>\n",
       "      <th>out</th>\n",
       "      <th>of</th>\n",
       "      <th>the</th>\n",
       "      <th>garret</th>\n",
       "      <th>which</th>\n",
       "      <th>he</th>\n",
       "      <th>lodged</th>\n",
       "      <th>S.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sent0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sent8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       On  an  exceptionally  hot  evening  early  in  July  a  young  man  \\\n",
       "sent0   1   1              1    1        1      1   1     1  1      1    1   \n",
       "sent1   0   0              0    0        0      0   1     0  0      0    0   \n",
       "sent2   0   0              0    0        0      0   0     0  0      0    0   \n",
       "sent3   0   0              0    0        0      0   0     0  1      0    0   \n",
       "sent4   0   0              0    0        0      0   0     0  1      0    0   \n",
       "sent5   0   0              0    0        0      0   0     0  0      0    0   \n",
       "sent6   0   0              0    0        0      0   0     0  0      0    0   \n",
       "sent7   0   0              0    0        0      0   0     0  1      1    1   \n",
       "sent8   0   0              0    0        0      0   1     0  0      0    0   \n",
       "\n",
       "       came  out  of  the  garret  which  he  lodged  S.  \n",
       "sent0     1    1   1    1       1      1   1       1   1  \n",
       "sent1     0    0   0    0       0      0   0       0   0  \n",
       "sent2     0    0   0    1       0      0   0       0   0  \n",
       "sent3     0    0   1    1       1      0   0       0   0  \n",
       "sent4     0    0   0    0       0      0   0       0   0  \n",
       "sent5     0    0   0    1       0      0   0       0   0  \n",
       "sent6     0    1   1    1       0      1   1       0   0  \n",
       "sent7     0    0   0    1       0      1   1       0   0  \n",
       "sent8     0    0   1    0       0      0   0       0   0  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus[df_corpus.columns[:20]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see there are some overlap in word usage for these sentences. If you want to compare documents or search for similar documents you need to compute this overlap.\n",
    "\n",
    "Using dot product we can do these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is _Dot Product_?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- _Dot Product_ is very important in machine learning and NLP. It is also called _inner product_ because \"inner\" dimension of the two vector(number of elements in each vector) or the matrix ( means _rows_ in matrix and _columns_ in other) must be equal.\n",
    "- It is also called _Scalar Product_ because after calculation you will get single scalar output.\n",
    "- For more yWatch [Linear Algebra(Gilbert Strang)] (https://www.youtube.com/playlist?list=PL49CF3715CB9EF31D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 44 44\n"
     ]
    }
   ],
   "source": [
    "# create to vector\n",
    "vec1 = np.array([1, 3, 5])\n",
    "vec2 = np.array([2, 4, 6])\n",
    " # do Dot product\n",
    "prod1 = vec1.dot(vec2)\n",
    "prod2 = (vec1 * vec2).sum()\n",
    "prod3 = vec1@vec2\n",
    "print(prod1, prod2, prod3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5],\n",
       "       [ 6,  7,  8,  9, 10],\n",
       "       [11, 12, 13, 14, 15],\n",
       "       [16, 17, 18, 19, 20]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix dor product\n",
    "\n",
    "mat1 = np.arange(1, 21).reshape(4,5)\n",
    "mat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  3,  4,  5,  6],\n",
       "       [ 7,  8,  9, 10, 11],\n",
       "       [12, 13, 14, 15, 16],\n",
       "       [17, 18, 19, 20, 21]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat2 = np.arange(2,22).reshape(4,5)\n",
    "mat2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[448, 482, 516, 550, 584],\n",
       "       [486, 524, 562, 600, 638],\n",
       "       [524, 566, 608, 650, 692],\n",
       "       [562, 608, 654, 700, 746],\n",
       "       [600, 650, 700, 750, 800]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_product = np.matmul(mat1.T, mat2)\n",
    "matrix_product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ok, enough math had done, now the application\n",
    "df_corpus = df_corpus.T\n",
    "df_corpus.sent0.dot(df_corpus.sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.sent0.dot(df_corpus.sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.sent0.dot(df_corpus.sent3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corpus.sent0.dot(df_corpus.sent6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is happening? \n",
    "Just the number of 1's in the dataframe df_corpus you will get the idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('out', 1), ('of', 1), ('the', 1), ('which', 1), ('he', 1)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# how to find the words that is shared by sent0 and sent 1?\n",
    "[(key, val) for (key, val) in (df_corpus.sent0 & df_corpus.sent6).items() if val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize sentence with regex\n",
    "import re\n",
    "sentence = \"\"\"He had not far to go; he knew indeed how many steps it was from the gate of his lodging house: exactly seven hundred and thirty. He had counted them once when he had been lost in dreams. At the time he had put no faith in those dreams and was only tantalising himself by their hideous but daring recklessness. Now, a month later, he had begun to look upon them differently, and, in spite of the monologues in which he jeered at his own impotence and indecision, he had involuntarily come to regard this ‚Äúhideous‚Äù dream as an exploit to be attempted, although he still did not realise this himself. He was positively going now for a ‚Äúrehearsal‚Äù of his project, and at every step his excitement grew more and more violent.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He',\n",
       " 'had',\n",
       " 'not',\n",
       " 'far',\n",
       " 'to',\n",
       " 'go',\n",
       " 'he',\n",
       " 'knew',\n",
       " 'indeed',\n",
       " 'how',\n",
       " 'many',\n",
       " 'steps',\n",
       " 'it',\n",
       " 'was',\n",
       " 'from',\n",
       " 'the',\n",
       " 'gate',\n",
       " 'of',\n",
       " 'his',\n",
       " 'lodging',\n",
       " 'house:',\n",
       " 'exactly',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'thirty',\n",
       " 'He',\n",
       " 'had',\n",
       " 'counted',\n",
       " 'them',\n",
       " 'once',\n",
       " 'when',\n",
       " 'he',\n",
       " 'had',\n",
       " 'been',\n",
       " 'lost',\n",
       " 'in',\n",
       " 'dreams',\n",
       " 'At',\n",
       " 'the',\n",
       " 'time',\n",
       " 'he',\n",
       " 'had',\n",
       " 'put',\n",
       " 'no',\n",
       " 'faith',\n",
       " 'in',\n",
       " 'those',\n",
       " 'dreams',\n",
       " 'and',\n",
       " 'was',\n",
       " 'only',\n",
       " 'tantalising',\n",
       " 'himself',\n",
       " 'by',\n",
       " 'their',\n",
       " 'hideous',\n",
       " 'but',\n",
       " 'daring',\n",
       " 'recklessness',\n",
       " 'Now',\n",
       " 'a',\n",
       " 'month',\n",
       " 'later',\n",
       " 'he',\n",
       " 'had',\n",
       " 'begun',\n",
       " 'to',\n",
       " 'look',\n",
       " 'upon',\n",
       " 'them',\n",
       " 'differently',\n",
       " 'and',\n",
       " 'in',\n",
       " 'spite',\n",
       " 'of',\n",
       " 'the',\n",
       " 'monologues',\n",
       " 'in',\n",
       " 'which',\n",
       " 'he',\n",
       " 'jeered',\n",
       " 'at',\n",
       " 'his',\n",
       " 'own',\n",
       " 'impotence',\n",
       " 'and',\n",
       " 'indecision',\n",
       " 'he',\n",
       " 'had',\n",
       " 'involuntarily',\n",
       " 'come',\n",
       " 'to',\n",
       " 'regard',\n",
       " 'this',\n",
       " '‚Äúhideous‚Äù',\n",
       " 'dream',\n",
       " 'as',\n",
       " 'an',\n",
       " 'exploit',\n",
       " 'to',\n",
       " 'be',\n",
       " 'attempted',\n",
       " 'although',\n",
       " 'he',\n",
       " 'still',\n",
       " 'did',\n",
       " 'not',\n",
       " 'realise',\n",
       " 'this',\n",
       " 'himself',\n",
       " 'He',\n",
       " 'was',\n",
       " 'positively',\n",
       " 'going',\n",
       " 'now',\n",
       " 'for',\n",
       " 'a',\n",
       " '‚Äúrehearsal‚Äù',\n",
       " 'of',\n",
       " 'his',\n",
       " 'project',\n",
       " 'and',\n",
       " 'at',\n",
       " 'every',\n",
       " 'step',\n",
       " 'his',\n",
       " 'excitement',\n",
       " 'grew',\n",
       " 'more',\n",
       " 'and',\n",
       " 'more',\n",
       " 'violent',\n",
       " '']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = re.split(r'[-\\s.,;?\"\"]+', sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going',\n",
       " 'now',\n",
       " 'for',\n",
       " 'a',\n",
       " '‚Äúrehearsal‚Äù',\n",
       " 'of',\n",
       " 'his',\n",
       " 'project',\n",
       " 'and',\n",
       " 'at',\n",
       " 'every',\n",
       " 'step',\n",
       " 'his',\n",
       " 'excitement',\n",
       " 'grew',\n",
       " 'more',\n",
       " 'and',\n",
       " 'more',\n",
       " 'violent',\n",
       " '']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pattern\n",
    "pattern = re.compile(r'[-\\s.,;!\"?]+')\n",
    "tokens = pattern.split(sentence)\n",
    "tokens[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer library\n",
    "There are several tokenizer library availables\n",
    "1. NLTK --Python\n",
    "2. spaCy --Python\n",
    "3. Stanford CoreNLP -- Python API but Java 8 dependency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positively',\n",
       " 'going',\n",
       " 'now',\n",
       " 'for',\n",
       " 'a',\n",
       " 'rehearsal',\n",
       " 'of',\n",
       " 'his',\n",
       " 'project',\n",
       " 'and',\n",
       " 'at',\n",
       " 'every',\n",
       " 'step',\n",
       " 'his',\n",
       " 'excitement',\n",
       " 'grew',\n",
       " 'more',\n",
       " 'and',\n",
       " 'more',\n",
       " 'violent']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+|$[0-9.]+|S+')\n",
    "tokenizer.tokenize(sentence)[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "sentence = \"\"\"The Chernobyl disaster was a nuclear accident that occurred on 26 April 1986 at the No. 4 reactor in the Chernobyl Nuclear Power Plant, near the city of Pripyat in the north of the Ukrainian SSR in the Soviet Union.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Power',\n",
       " 'Plant',\n",
       " ',',\n",
       " 'near',\n",
       " 'the',\n",
       " 'city',\n",
       " 'of',\n",
       " 'Pripyat',\n",
       " 'in',\n",
       " 'the',\n",
       " 'north',\n",
       " 'of',\n",
       " 'the',\n",
       " 'Ukrainian',\n",
       " 'SSR',\n",
       " 'in',\n",
       " 'the',\n",
       " 'Soviet',\n",
       " 'Union',\n",
       " '.']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(sentence)[-20:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to go to Extending Vocabulary to learn about N-gram and other cool stuffs."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2e753d71ef7304ddfd93a48aa366aa4050d26dee3a9358c3d90e9e84935e3962"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
